# DistilRoBERTa embeddings + Logistic Regression (same LR pipeline as MiniLM)
# Use: python scripts/train.py --task intent --encoder minilm_lr --config configs/distilroberta_lr.yaml
#      python scripts/train.py --task concern --encoder minilm_lr --config configs/distilroberta_lr.yaml
# Then set pipeline retriever encoder_model to the same model and rebuild KB if you want consistent embeddings.

data:
  data_cfg: configs/data.yaml

model:
  embedder: sentence-transformers/all-distilroberta-v1  # 768-d, DistilRoBERTa

training:
  max_iter: 200
  C: 0.5
  class_weight: balanced
  threshold: 0.5
  n_jobs: -1
  seed: 42

logging:
  run_name: distilroberta_lr
  save_dir: results/runs
