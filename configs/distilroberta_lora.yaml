# configs/distilroberta_lora.yaml

# Path to the shared data configuration
data:
  data_cfg: configs/data.yaml

# Model configuration
model:
  name: "distilroberta-base"
  max_length: 512
  num_labels: 9

# LoRA (Low-Rank Adaptation) configuration
lora:
  r: 64
  lora_alpha: 128
  lora_dropout: 0.1
  target_modules:
    # - "q_lin"
    # - "v_lin"
    - "roberta.encoder.layer.0.attention.self.query"
    - "roberta.encoder.layer.0.attention.self.key"
    - "roberta.encoder.layer.0.attention.self.value"
    - "roberta.encoder.layer.1.attention.self.query"
    - "roberta.encoder.layer.1.attention.self.key"
    - "roberta.encoder.layer.1.attention.self.value"
    - "roberta.encoder.layer.2.attention.self.query"
    - "roberta.encoder.layer.2.attention.self.key"
    - "roberta.encoder.layer.2.attention.self.value"
    - "roberta.encoder.layer.3.attention.self.query"
    - "roberta.encoder.layer.3.attention.self.key"
    - "roberta.encoder.layer.3.attention.self.value"
    - "roberta.encoder.layer.4.attention.self.query"
    - "roberta.encoder.layer.4.attention.self.key"
    - "roberta.encoder.layer.4.attention.self.value"
    - "roberta.encoder.layer.5.attention.self.query"
    - "roberta.encoder.layer.5.attention.self.key"
    - "roberta.encoder.layer.5.attention.self.value"

# Training hyperparameters
training:
  seed: 42
  threshold: 0.25
  epochs: 10
  train_batch_size: 8
  eval_batch_size: 32
  learning_rate: 1e-4
  weight_decay: 0.01
  output_dir: "models/checkpoints/intent/"

# Logging configuration
logging:
  run_name: "distilroberta_base_lora"
  save_dir: "results/runs"
